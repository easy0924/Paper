\documentclass[a4paper]{article}
\usepackage{iwslt14,amssymb,amsmath,epsfig,multirow,spreadtab,array}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{trees}
\usetikzlibrary{shapes}
\usetikzlibrary{chains}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usepackage[T1]{fontenc}
\usepackage[latin1,utf8x]{inputenc}
\usepackage[encapsulated]{CJK}
\newcommand{\cntext}[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}
\setcounter{page}{1}
\sloppy		% better line breaks
%\ninept
%SM below a registered trademark definition
\def\reg{{\rm\ooalign{\hfil
     \raise.07ex\hbox{\scriptsize R}\hfil\crcr\mathhexbox20D}}}

%% \newcommand{\reg}{\textsuperscript{\textcircled{\textsc r}}}

\title{Rule-Based Preordering on
Multiple Syntactic Levels \\in Statistical Machine Translation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please make sure to keep technical paper submissions anonymous  !
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\name{Firstname Lastname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \makeatletter
 \def\name#1{\gdef\@name{#1\\}}
 \makeatother
% \name{{\em Ge Wu, Yuqi Zhang, Alexander Waibel}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

%\address{Institute for Anthropomatics \\
%Karlsruhe Institute of Technology, Germany \\
%{\small \tt utcur@student.kit.edu, yuqi.zhang@kit.edu, alexander.waibel@kit.edu }
%}

\begin{document}
\maketitle
%
\begin{abstract}
We propose a novel data-driven rule-based preordering approach, which uses the tree information of multiple syntactic levels. This approach extend the tree-based reordering from one level into multiple levels, which has the capability to process more complicated reordering cases. We have conducted experiments in English-to-Chinese and Chinese-to-English translation directions. Our results show that the approach has led to improved translation quality both when it was applied separately or when it was combined with some other reordering approaches. As our reordering approach was used alone, it showed an improvement of $1.61$ in BLEU score in the English-to-Chinese translation direction and an improvement of $2.16$ in BLEU score in the Chinese-to-English translation direction, in comparison with the baseline, which used no word reordering. As our preordering approach were combined with the short rule \cite{short}, long rule \cite{long} and tree rule \cite{tree} based preordering approaches, it showed further improvements of up to $0.43$ in BLEU score in the English-to-Chinese translation direction and further improvements of up to $0.3$ in BLEU score in the Chinese-to-English translation direction. Through the translations that used our preordering approach, we have also found many translation examples with improved syntactic structures.

%syntax tree and word alignment to reorder the words in source sentences before decoding in a phrase-based SMT system between English and Chinese. The preordering algorithm extracts reordering patterns from multiple levels of the syntax trees of the training data and applies the rules on the source sentences in a similar way.
 

\end{abstract}
%
\section{Introduction}
\label{in}

Word order is a general issue when we want to translate text from one language to another. Different languages normally have different word orders and the difference could be very huge. Among all the languages, Chinese is one language which is very different from English, because they belong to different language families and have long period of separately development. Both languages have a Subject-Verb-Object order, but they also have a lot of differences in word order. Especially sentences in both languages can sometimes have completely different syntactic structures. The differences may involve long-distance or unstructured position changes.

Most state-of-the-art phrase-based SMT systems use language model, phrase table or decoder to adjust the word order. Or an additional reordering model is used in the log-linear model for word reordering. However, these methods may have some disadvantages, such as some do not handle long-distance reordering, some do not handle unstructured reordering and some are rather time-consuming.

Encouraged by the results from the result from Rottmann and Vogel 2007 \cite{short}, Niehues and Kolss 2009 \cite{long} and Herr\-mann et al. 2013 \cite{tree}, we further propose a new data-driven, rule-based preordering method, which extracts and applies reordering rules based on syntax tree. The method is called Multi-Level-Tree (MLT) reordering, which orders the constituents on multiple levels of the syntax tree all together. This preordering method rearranges the words in source language into a similar order as they are supposed to be in the target language before translation. With the proper word order, better translation quality can be achieved. Especially, our preordering method is very suitable for translation between language pairs like English and Chinese, which have very different word orders. Besides, the method can also be combined with the above mentioned rule-based reordering methods to achieve better translation quality. 

The rest of this paper is organized as follows. Section~\ref{re} presents a review of related work. In Section~\ref{mo}, we point out the problems for translation between English and Chinese and describe the motivation of MLT reordering. In Section~\ref{mu}, we introduce the details of the MLT reordering. In Section~\ref{ex}, we present the experimental results and evaluation. Finally, we conclude this paper in Section~\ref{co}.

\section{Related Work}
\label{re}

Word reordering is an important problem for statistical machine translation, which has long been addressed.

In a phrase-based SMT system, there are several possibilities to change the word orders. Words can be reordered during the decoding phase by setting a window, which allows the decoder to choose the next word for translation. Reordering could also be influenced by the language model, because the language model gives probability of how a certain word is likely to follow. Different language model may give different probability, which further influences the decision made by log-linear model. Other ways to change the word orders include using distance based reordering models or lexicalized reordering models \cite{tillmann2004, koehn2005}. The lexicalized reordering model reorders the phrases by using information of how the neighboring phrases change orientations.

Another way to achieve word reordering is to detach it from decoding phase and do it separately in a pre-process before decoding, in order to reduce the time for translation. This kind of preordering approaches use linguistic information to modify the word orders. Preordering can also be rule-based, which extracts different types of reordering rules by observing reordering patterns from the training data and apply the rules to the sentences to be translated. Depends on how the rules are defined, different information may be used such as word alignments, POS tags, syntax trees, etc.

Some early approaches use manually defined reordering rules based on the linguistic information for particular languages \cite{collins2005clause, popovic2006pos,habash2007syntactic,syntactic}. Later come the data-driven methods \cite{zhang2007chunk, crego2008using}, which learn the reordering rules automatically. 

Rottmann and Vogel 2007 \cite{short} introduced the idea of extracting reordering rules from the POS tag sequences of training data and use them for reordering. Niehues and Kolss 2009 \cite{long} went further, and developed a method for long-distance word reordering, which works good on German-English translation task due to the long-distance shift of verbs. The method extracts discontinuous reordering rules in addition to the continuous ones, which uses a placeholder to match several words and enables the word to shift cross long distance.

Afterwards, Herrmann et al. 2013 \cite{tree} introduced a new approach to reorder the words based on syntax tree, which leads to further improvements on translation quality. The algorithm takes syntactic structure of the sentences into account and extract the rules from the syntax tree by detecting the reordering of child sequences. It also has the variant based only on part of the child sequences which is suitable for language with flat syntactic structure such as German.

However, these approaches which are based on POS tag sequences or syntax trees are mostly designed for languages like German and are not especially adapted for languages like Chinese. As Chinese has very different word orders, a reordering approach, which can further explore the syntactic structure of Chinese and utilize this information for reordering, is desirable.

The hierarchical phrase-based translation model \cite{hier} is especially suitable for Chinese translation, and provide very good translation results. It extracts hierarchical rules by using information of the syntactic structure. Phrases from different hierarchies, or so-called phrases of phrases, are reordered during the decoding. 

The idea of phrases on different hierarchies has inspired us to create this preordering method based on multiple levels of the syntax tree. Besides, we also hope to detach the reordering from decoding phase and do it separately in a pre-process before decoding, in order to reduce the time for translation. This kind of preordering approaches use linguistic information to modify the word orders.

Oracle reordering has also shown values for evaluating the potential of preordering. \cite{metrics} introduced the permutation distance metrics which can be used to measure reordering quality. And \cite{birch2} described how we can construct permutations from the word alignment as oracle reordering.

In this paper, we compare our results to the aforementioned rule-based reordering methods and the oracle reordering to get a better overview.

\section{Motivation}
\label{mo}

The word order between English and Chinese differs very much. For one, the words in Chinese have generally different origins as those in English, which leads to very different vocabulary and word construction. Sometimes it is very hard to find corresponding words in the other language. For example, some prepositions in Chinese have very different usage than those prepositions in English. Also the continuous writing of Chinese without spaces makes this problem more severe, since word boundaries are not always so clear in Chinese. The text needs to be segmented first before translation. A word segmentation process is used to separate the words, but the results may not always be ideal.

For the other, both languages have sometimes very different sentence structures. Thus, a word-for-word translation between English and Chinese is often unnatural or difficult to understand. Each of them has some sentence patterns that do not exist or rarely used in the other. In Chinese, a modifier is often put before the part that it modifies. While in English, it is very common that the modifier is put after the part that it modifies. Besides, English sentences with a lot of long clauses may be more suitable to translate into several Chinese sentences, because in Chinese people do not tend to use long clauses in general. 

Some typical problems of word orders between English and Chinese that we have found are as follows:

\begin{itemize}
\item \emph{Pre-modifier instead of post-modifier}\\ In Chinese people tend to use pre-modifier rather than post-modifier. This involves the position change of adverbials, relative clauses and preposition phrases during translation. Figure~\ref{relative} shows an example of how the position of a relative clause changes
\begin{figure}
\input{relative.tikz}
\caption{Position change of a relative clause}
\label{relative}
\end{figure}
\item \emph{Construction of questions}\\
The two languages have very different ways to construct questions, which raises word order problems for translating questions. Figure~\ref{question} shows word reordering of a question.
\begin{figure}
\input{question.tikz}
\caption{Word reordering of a question}
\label{question}
\end{figure}
\item \emph{Special sentence constructions}\\ For example, \emph{B\^{a}}-construction (\cntext{把字句}) in Chinese and sentence constructions such as \emph{there-be} and inverted negative sentences in English do not have correspondence in the other language in general.
\item \emph{Long distance word position change}\\
Word reordering between English and Chinese often involves word shift cross long distance. For example, following translation shows that a adverbial clause (underlined) is shifted cross long distance when being translated.\bigskip \\
\parbox{0.3705\textwidth}{
I find this very much disturbing \uline{when we are talking about what is going on right and wrong with democracy these days}.\medskip\\
\cntext{\uline{现在，每当我跟别人讨论我们的民主什么是}}
\cntext{\uline{对的，什么是错的}我都为此觉得很无力。}
}
\end{itemize}

Following the analysis above, in order to improve the reordering between English and Chinese, we need reordering method that can handle more complicated, unstructural word order change. Inspired by the ideas of reordering on syntax tree and hierarchical phrases, we created the Multiple-Level-Tree(MLT) reordering, which reorders words based on multiple syntactic levels and can handle long distance word position change and complicated word position change very well.

\section{Multi-Level-Tree Reordering}
\label{mu}

Our preordering method is based on automatically learned reordering rules. Reordering rules show how sentences should be reordered in source language before translation. In our system, the rules are generated by using the word alignment, and syntax tree, all of which are calculated based on the training data. After reordering rules are applied to the source sentences, word lattices are generated. A word lattice contains all the reordering possibilities of a source sentence and is further passed to the decoder for translation. The preordering system could be illustrated as Figure~\ref{prereordering}.

\begin{figure*}
\centering
\tikzset{every picture/.style={scale=0.8}}%
\input{prereordering.tikz}
\caption{Illustration of preordering system}
\label{prereordering}
\end{figure*}

\subsection{Reordering on Multiple Syntactic Levels}

Reordering patterns are based on multiple levels of the syntax tree. Figure~\ref{unstructured} illustrates how the reordering patterns are detected from the syntax tree. In the example, the detection starts from the root node, go downwards three levels and use the nodes in these levels to detect the reordering pattern. These nodes that are used for detecting the reordering pattern are colored gray and have an italic font. The nodes with dark gray are the lowest ones among such nodes, which represent constituents that are actually reordered through the reordering rules. The leaf nodes in the syntax tree indicate words in the sentence, which has rectangle shape with angular corners.

According to the alignment information with the corresponding Chinese translation, the node labeled with \emph{NP} is moved forward to the first place in the translation and the node labeled with \emph{IN-of} is moved forward to the second place in translation. This reordering cannot be handled with one-level tree-based reordering, but in MLT, from the root node with a search depth of three, the following reordering pattern can be found:\bigskip

\noindent \texttt{NP(\textbf{CD}$_0$ NP(NP(\textbf{JJ}$_1$ \textbf{NNS}$_2$)PP(\textbf{IN}$_3$ \textbf{NP}$_4$)))}\medskip\\
\texttt{-> NP IN CD JJ NNS}\\
\texttt{-> 4 3 0 1 2 (alternative with index)}\bigskip

The POS tags in bold corresponds to nodes with dark gray in Figure~\ref{unstructured}, which presents constituents that are actually reordered. The parentheses in the reordering pattern indicate the corresponding hierarchies in the syntax tree. The reordering can be alternatively represented with index, which is the actual internal representation to avoid ambiguity caused by POS tags with the same name and we use this representation throughout this paper.

\begin{figure}[ht]
\centering
%\tikzset{every picture/.style={scale=0.8}}%
\input{unstructured.tikz}
\caption{Detection of reordering pattern from multiple syntactic levels}
\label{unstructured}
\end{figure}

\subsection{Rule Extraction}

In order to find as much information for reordering as possible, the algorithm of rule extraction detects the reordering patterns from all nodes in the syntax tree and it goes downwards for any number of hierarchies, until it reaches the lowest hierarchy in the subtrees.

In the implementation, the program conducts a depth-first search (DFS) to traverse every node in a syntax tree. Every time when a node is reached, the program conducts another iterative deepening depth-first search (IDDFS) in its subtree with depth-limit from $1$ to the subtree's depth. And the program detects if there are any patterns of word position changes at the same time, by using the alignment for comparison.

The detected word position changes are checked for their validity for reordering rules. A valid reordering pattern should both has actual word reordering and clearly distinguishable new order on the side of the target language, i.e. no collision of aligned ranges on the target side.

\begin{figure}[ht]
\centering
\input{extract.tikz}
\caption{A phrase with its syntax tree and word alignment for rule extraction}
\label{extract}
\end{figure}

Figure~\ref{extract} shows a phrase to be translated, together with its syntax tree and word alignment of parallel text. In this example, we can find the following reordering patterns:\bigskip

\noindent From node $1$:\\
\texttt{NP(\textbf{NP} \textbf{PP})->1 0} \hfill [1 level]\hphantom{s}\\
\texttt{NP(NP(\textbf{JJ} \textbf{NNS})PP(\textbf{IN} \textbf{NP}))->3 2 0 1} \hfill [2 levels]\\
\texttt{NP(NP(\textbf{JJ} \textbf{NNS})PP(\textbf{IN} NP(\textbf{JJ} \textbf{NNS})))-> \\ 3 4 2 0 1} \hfill [3 levels]\medskip

\noindent From node $3$:\\
\texttt{PP(\textbf{IN} \textbf{NP})->1 0} \hfill [1 level]\hphantom{s}\\
\texttt{PP(\textbf{IN} NP(\textbf{JJ} \textbf{NNS}))->1 2 0} \hfill [2 levels]\bigskip

The probability of the reordering patterns are calculated based on frequency of their occurrences in the training corpus. There are the left part and the right part of the reordering patterns separated by the arrow. The left part indicates the syntactic tags that should be reordered and the right part indicates how the new order should be like. The probability of the pattern is calculated by how often the left part is reordered into the right part among all its appearances in the training corpus. In addition, reordering patterns that appear less than a threshold are ignored to be used as reordering rules, in order to prevent too concrete rules without generalization capability and overfitting.

\subsection{Rule Application}

The syntax tree is traversed by DFS as the same in rule extraction. But from the root of each subtree, it has scanned with depth limit from its maximal levels, i.e. its depth, to $1$. As it turns out that any rule can be applied for a subtree at some level, a new path for this reordering will be added to the word lattice for decoding. As long as rules can be applied on a subtree for a certain depth, the search for rule application on this subtree stops, and the search on the next subtree continues.

The reason for this is to prevent duplicate reorderings due to application of nested rules, which have overlapped effect with each other. These rules are normally patterns that are generated on the same subtree, but with different number of levels, which has different generalization effect on the same range of words in the text. For example, the following patterns can be detected from the syntax tree in Figure~\ref{extract}:\bigskip

\noindent \texttt{PP(\textbf{IN} \textbf{NP}) -> 1 0}\\
\texttt{PP(\textbf{IN} NP(\textbf{JJ} \textbf{NNS})) -> 1 2 0}\bigskip

Both patterns are detected from the same node, but the second pattern is detected by retrieving the nodes one level deeper and it is more concrete. So the first pattern can be seen as a generalization of the second pattern. Whenever a rule of the second pattern can be applied, a rule of the first pattern can be applied too. Because subtrees are checked from the highest number of levels in rule application, the more concrete rule is applied first. Since the more concrete rule fits the detected pattern better and contains more details of reordering, so it may be more suitable for rule application. In this example, the second rule is applied rather than the first rule.

Word reorderings are added to the word lattice as paths, which is further past to the decoder for translation. Paths with very low probability are removed, in order to save space for storing the lattice and reduce decoding time later, without compromising too much translation quality. 

\subsection{Rule Combination}

In order to further explore the probability of improvement, the MLT reordering rules can be combined with other types of reordering rules to achieve further improvements. This is done by training the different types of rules separately and applying them on the monotone path of the sentence independently. All the generated different paths are compressed in the word lattice.

\section{Experiments}
\label{ex}

We have conducted experiments on both English-to-Chinese and Chinese-to-English translation directions to get a better overview of the MLT reordering's effect. In the experiments, the MLT reordering rules are also combined with other reordering rules that are introduced before, in order to show the improvement achieved by our approach.

In order to evaluate the potential of word reordering, we also used oracle reordering in the experiments. Oracle reordering is considered to be an optimally reordered sentence as input to the SMT system and do not allow additional reordering during decoding \cite{combine}. So we can use it as input of the SMT system and the scores are the optimal results that can be achieved by word reordering, from which we can evaluate the potential of reordering methods.


\subsection{English-to-Chinese System}

We performed experiments with and without different reordering methods covering the English-to-Chinese translation direction. The reordering methods included our MLT reordering approach and the other reordering approaches with short rules, long rules and tree rules. The system was trained on news text from the LDC corpus and subtitles from TED talks. The development data and test data were both news text from the LDC corpus. The system was a phrase-based SMT system, which used a $6$-gram language model with Knersey-Ney smoothing. Besides the preordering, no lexical reordering or other reordering method in decoding phase was used. The text was translated through a monotone decoder.

The reordering rules were extracted by using the word alignments, POS tags and syntax trees from the training data. One reference of the test data was used for evaluating the results. The threshold for rule extraction is set as $5$ times and reordering paths with probability less than $0.1$ are not added to word lattices. The decoder was a monotone decoder. Table~\ref{denw} shows the size of data used in the system.

\begin{table*}
\centering
\begin{tabular}{|ll|r|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{Data Set}} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{4-7}
& & & English & Chinese & English & Chinese \\
\hline
\multirow{2}{*}{Training Data} & \multicolumn{1}{|l|}{LDC} & 303K & 10.96M & 8.56M & 60.88M & 47.27M \\ \cline{2-7}
& \multicolumn{1}{|l|}{TED} & 151K & 2.58M & 2.86M & 14.24M & 15.63K \\ \hline
\multicolumn{2}{|l|}{Development Data} & 919 & 30K & 25K & 164K & 142K \\ \hline
\multicolumn{2}{|l|}{Test Data} & 1663 & 47K & 38K & 263K & 220K \\ \hline
\end{tabular}
\caption{Data details in English-to-Chinese system}
\label{denw}
\end{table*}

\begin{table}
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|r|}}\hline
@				& @BLEU (\%) & @ Imprv. & @TER (\%) \\ \hline
@Baseline		& 12.07 & & 72.15 \\ \hline
@+Short Rules	& 12.50 & :={b3 - b2} & 71.41 \\ \hline
@+Long Rules   & 12.99 & :={b4 - b2} & 70.71 \\ \hline
@+Tree Rules   & 13.38 & :={b5 - b2} & 68.27 \\ \hline
\textbf{@+MLT Rules} & \textbf{:={13.81}} & \textbf{:={b6 - b2}} & \hphantom{xxx}\textbf{:={68.20}} \\ \hline
@Oracle Reordering & :={18.58} & :={b7 - b2} & :={62.13} \\ \hline
\hline
@Long Rules   & 12.31 & :={b8 - b2} & 71.81\\ \hline
@Tree Rules   & 13.30 & :={b9 - b2} & 70.42 \\ \hline
\textbf{@MLT Rules}    & \textbf{:={13.68}} & \textbf{:={b10 - b2}} & \textbf{:={70.25}} \\ \hline
\end{spreadtab}
\caption{Result overview of English-to-Chinese system}
\label{tenw}
\end{table}

Table~\ref{tenw} shows the BLEU scores, absolute improvements of BLEU scores and TER scores for configurations with different reordering methods. The table consists of $2$ sections. the first row of the top section shows results of the baseline, which used no preordering at all. In the following rows of the top section, different types of reordering rules are combined gradually, with each type per row. For example, the row with \emph{$+$MLT Rules} presents the configuration with all the rule types including MLT rules and all the other rules in the rows above. All improvements are absolute improvements of BLEU scores in comparison to the baseline. Each row with a certain reordering type presents all the different variations of this type and the best score under these configurations is shown. For example, long rules include the left rules and right rules, and the tree rules include the partial rules and recursive application. The baseline used a monotone decoder and no preordering. The row with \emph{oracle reordering} shows the results from the configuration that used the oracle reordering as input. The results of oracle reordering can be used for analyzing the potential of source sentence reordering. In the lower section of the table, different rule types are not combined and the effect of each rule type is shown.

\subsection{Chinese-to-English System}

The experiments for Chinese-to-English systems have a similar setup as described in the last section. The parallel data used in the English-to-Chinese system was also used in this experiment by switching the source language and the target language. We only used the LDC data set for training, and no TED data were used in this system. The test data had three English references for evaluating the results instead of one as in the previous system. The data used are summarized in Table~\ref{dzhen2}.

\begin{table*}
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multirow{2}{*}{Data Set} & \multirow{2}{*}{Sentence Count} & \multicolumn{2}{c|}{Word Count} & \multicolumn{2}{c|}{Size (Byte)}\\ \cline{3-6}
& & Chinese & English & Chinese & English \\
\hline
Training Data & 303K & 8.56M & 10.96M & 47.27M & 60.88M\\ \hline
Development Data & 919 & 25K & 30K & 142K & 164K \\ \hline
Test Data & 1663 & 38K & 47K & 220K & 263K \\ \hline
\end{tabular}
\caption{Data details in Chinese-to-English system}
\label{dzhen2}
\end{table*}

\begin{table}
\centering
\STautoround*{2}
\begin{spreadtab}{{tabular}{|l|r|r|r|}}\hline
@				& @BLEU (\%) & @Imprv. & @TER (\%) \\ \hline
@Baseline		& 21.80 & & 62.09 \\ \hline
@+Short Rules	& 22.90 & :={b3 - b2}& 61.64 \\ \hline
@+Long Rules   & 23.13 & :={b4 - b2} & 61.43\\ \hline
@+Tree Rules   & 23.84 & :={b5 - b2} & 60.95\\ \hline
\textbf{@+MLT Rules}    & \textbf{:={24.14}} & \textbf{:={b6 - b2}} & \hphantom{xxx} \textbf{:={60.79}}\\ \hline
@Oracle Reordering & :={26.80} & :={b7 -b2} & :={56.97} \\ \hline
\hline
@Long Rules   & 22.10 & :={b8 - b2} & 62.21\\ \hline
@Tree Rules   & 23.35 & :={b9 - b2} & 61.52\\ \hline
\textbf{@MLT Rules}    & \textbf{:={23.96}} & \textbf{:={b10 - b2}} & \textbf{:={60.83}}\\ \hline
\end{spreadtab}
\caption{Result overview of Chinese-to-English systems}
\label{tzhen2}
\end{table}

Table~\ref{tzhen2} shows the results for configurations with different reordering methods for the Chinese-to-English translation. The table can be interpreted in the same manner as Table~\ref{tenw} in the previous section.

\subsection{Evaluation}

The results shows increasing scores when we used reordering methods from short rules, long rules, tree rules to MLT rules. And better BLEU scores were achieved when we combined the different reordering rules. The MLT rules achieved better BLEU scores and TER scores in both translation directions, not only when it was used alone, but also it was added to the other reordering rules. As the MLT reordering rules were used alone, it showed an improvement of $1.61$ in BLEU score in the English-to-Chinese translation direction and an improvement of $2.16$ in BLEU score in the Chinese-to-English translation direction, in comparison with the baseline, which used no reordering at all. As the MLT reordering rules were combined with the other existing reordering rules, a further improvement of $0.43$ in BLEU score (from $13.38$ to $13.81$) was shown in the English-to-Chinese translation direction, as well as a further improvement of $0.3$ in BLEU score (from $23.84$ to $24.14$) in the Chinese-to-English translation direction.

We have also found improvements in the sentence structure. Table~\ref{t1} and Table~\ref{t2} show some translation examples in both translation directions. Sections are separated by double lines in the table. Each section of this table shows one translation example with the source sentence (\emph{source}), translation without using MLT reordering (\emph{no MLT}), translation with MLT reordering (\emph{MLT}) and the reference (\emph{reference}). \emph{Glossary} for the source sentences or references in Chinese is also added as word for word translation. Each word or group of hyphenated words in the glossary corresponds a Chinese character or a group of Chinese characters that are not separated with space. A placeholder $\Box$ is used to replace words that are difficult to translate, which play grammatical roles generally. The translation without using MLT reordering comes from the configuration with highest BLEU score that did not use MLT reordering. And the translation with MLT reordering comes from the configuration with highest BLEU score that used MLT reordering. From the examples, we can clearly see the improvements in sentence structure.

% Each word or group of hyphenated words in the glossary corresponds a Chinese character or a group of Chinese characters that are not separated with space in the source sentence with the same order.

\begin{table*}
\centering
\begin{tabular}{|l|m{0.7\textwidth}|} \hline
Source & \cntext{陈至立 说 , 古巴 是 拉美 和 加勒比 地区 有 重要 影响 的 国家 。}
\\ \hline
Glossary & chen-zhili said , cuba is latin-american and caribbean region has great influence $\Box$ country .
\\ \hline
No MLT & chen zhili said : cuba is the latin america and the caribbean region has an important influence on the state .
\\ \hline
MLT & chen zhili said : cuba is a country of important influence latin america and the caribbean region .
\\ \hline
Reference & chen zhili said that cuba is a country of great influence in the latin american and caribbean region .
\\ \hline \hline

Source & \cntext{近年 来 , 两 国 教育 交流 日益 密切 , 人员 来往 频繁 。}
\\ \hline
Glossary & recent-years in, two countries educational exchange increasingly close , personnel visits frequent .
\\ \hline
no MLT & in recent years , the two countries education have been increasingly close exchanges and personnel contacts have been frequent .
\\ \hline
MLT & in recent years , the educational exchanges between the two countries have become increasingly frequent , and have had frequent contacts .
\\ \hline
Reference & in recent years , the educational exchange between the two countries has become increasingly close with frequent personnel visits .
\\ \hline
\end{tabular}
\caption{Examples of translations from Chinese to English}
\label{t1}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|l|m{0.7\textwidth}|} \hline
Source & hu jintao also extended deep condolences on the death of the chinese victims and expressed sincere sympathy to the bereaved families .
\\ \hline
No MLT & \cntext{胡锦涛 还 表示 深切 哀悼 的 受害者 家属 的 死亡 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline
MLT & \cntext{胡锦涛 还 对 中国 迂难者 表示 哀悼 , 向 迂难者 家属 表示 诚挚 的 慰问 。} \\ \hline 
Reference & \cntext{胡锦涛 还 对 中方 不幸 遇难 人员 表示 深切 的 哀悼 , 并 向 遇难者 的 亲属 致以 诚挚 的 慰问 。} \\ \hline
Glossary & hu-jintao also on chinese unfortunately killed people extended deep $\Box$ condolences , and to bereaved 's families expressed sincere $\Box$ sympathy .
\\ \hline \hline
Source & satisfying personal interests and expanding knowledge are also major reasons why hourly work appeals to people .\\ \hline
No MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 主要 原因 小时 工作 吸引 人 。} \\ \hline
MLT & \cntext{满足 个人 利益 和 扩大 知识 也 是 为什么 学生 工作 吸引 人 的 主要 原因 。} \\ \hline 
Reference & \cntext{满足 个人 兴趣 , 扩大 自己 的 知识面 也 是 兼职 小时 工 受 青睐 的 一 个 重要 原因 。}  \\ \hline
Glossary & satisfying personal interests , expanding own $\Box$ knowledge also are part-time hourly work is favored $\Box$ one $\Box$ major reason .
\\ \hline \hline

Source & the dalai lama will go to visit washington this month .\\ \hline
No MLT & \cntext{达赖 喇嘛 将 访问 华盛顿 的 这 一 个 月 。} \\ \hline
MLT & \cntext{达赖 喇嘛 将 本 月 访问 华盛顿 。} \\ \hline
%? add more examples?
Reference & \cntext{达赖 喇嘛 将 在 本 月 前往 华盛顿 访问 。} \\ \hline
Glossary & dalai lama will in this month go-to washington visit .
\\ \hline
\end{tabular}
\caption{Examples of translations from English to Chinese}
\label{t2}
\end{table*}

From these experiments we can draw the conclusion that our reordering method obviously improves the sentence structure and  translation quality in both English-to-Chinese and Chinese-to-English translation directions, no matter when we apply it alone or when we combine it with short rules, long rules and tree rules. 

\section{Conclusions}
\label{co}

We have presented a new preordering approach for translation
between English and Chinese. The algorithm detects and applies reordering rules by using information of multiple syntactic levels in the syntax tree and word alignment to reorder the source sentences. Reordering patterns are detected by checking if the nested tag sequences in subtrees with any number of search levels have clearly new orders in the aligned text in the target language.

We have conducted experiments in both translation directions between English and Chinese with different SMT configurations. From the results we can see the BLEU scores were improved no matter when we applied the SMT reordering method to the baseline directly or when we combined it with the other reordering methods, i.e. short rules, long rules and tree rules based reordering methods.

%When our approach was applied alone, it achieved an BLEU score improvement of $1.61$ in the English-to-Chinese translation direction, and an BLEU score improvement of $2.16$ in the Chinese-to-English translation direction.

%When our approach was combined with the short rules, long rules and tree rules based reordering methods, further improvements were achieved for both translation directions. Our approach improved the BLEU score further by $0.43$ in the English-to-Chinese translation direction and by $0.30$ in the Chinese-to-English translation direction.

Besides the improvement in BLEU scores, our preordering approach also showed improvement in the sentence structure of the translation. Considering sentences that have complicated structure only make up a small part of the data, even a small improvement in the result can mean a big improvement in translating these complicated sentences.

By taking a close look at the gap between the scores of oracle reordering and the best scores achieved by MLT reordering, we can also see, there is still potential for improvements of translation between English and Chinese through better reordering methods.

\bibliographystyle{myieeetr}
\bibliography{paper}
\end{document}

